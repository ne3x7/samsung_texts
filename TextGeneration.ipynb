{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, text generation will be performed using character-level LSTM.\n",
    "\n",
    "I used several sources of knowledge. Firstly, Google&Udacity [\"Deep Learning\" course](https://www.udacity.com/course/deep-learning--ud730) and Andrej Karpathy [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Secondly, [my own experience](https://github.com/ne3x7) of taking Deep NLP Course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had pretty clean concatenated Dostojevsky texts, so I decided to use this file as source. Since we are building a character-level LSTM, vocabulary is built on set of characters and no embedding is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    Reads source text file into memory.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): A file in ./data/ folder.\n",
    "        \n",
    "    Returns:\n",
    "        raw: A string representation of source text.\n",
    "    \"\"\"\n",
    "    assert os.path.exists('./data/%s' % filename), 'File %s not found in folder ./data' % filename\n",
    "    with open('./data/%s' % filename, 'r') as fin:\n",
    "        raw = fin.read()\n",
    "    return raw.lower()\n",
    "\n",
    "def build_corpus(data, valid_size_pct=0.1):\n",
    "    \"\"\"\n",
    "    Builds character-index mapping, performs trin/test split, creates corpus as a list of indexes.\n",
    "    \n",
    "    Args:\n",
    "        data (str): A string representation of source text.\n",
    "        valid_size_pct (float): Percentage of data to keep for validation.\n",
    "        \n",
    "    Returns:\n",
    "        vocabulary_size (int): Vocabulary size.\n",
    "        valid_size (int): Validatons set size.\n",
    "        char2id: A dict mapping characters to indexes.\n",
    "        id2char: A dict mapping indexes to characters.\n",
    "        train_corpus: An index representation of train text.\n",
    "        valid_corpus: An index representation of validation text.\n",
    "    \"\"\"\n",
    "    data = data.decode('utf-8')\n",
    "    total_size = len(data)\n",
    "    valid_size = int(valid_size_pct * total_size)\n",
    "    train_size = total_size - valid_size\n",
    "    valid_text = data[:valid_size]\n",
    "    train_text = data[valid_size:]\n",
    "    \n",
    "    print 'Using validation size %d' % valid_size\n",
    "    \n",
    "    valid_corpus = []\n",
    "    train_corpus = []\n",
    "    char2id = {}\n",
    "    id2char = {}\n",
    "    \n",
    "    vocabulary = set(data)\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    print 'Using vocabulary size %d' % vocabulary_size\n",
    "    \n",
    "    for i, c in enumerate(vocabulary):\n",
    "        char2id[c] = i\n",
    "        id2char[i] = c\n",
    "        \n",
    "    for c in train_text:\n",
    "        train_corpus.append(char2id[c])\n",
    "        \n",
    "    for c in valid_text:\n",
    "        valid_corpus.append(char2id[c])\n",
    "        \n",
    "    return vocabulary_size, valid_size, char2id, id2char, train_corpus, valid_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 11396681\n",
      "Example: 'Ох уж эти мне сказочники! Нет чтобы '\n",
      "Using validation size 63754\n",
      "Using vocabulary size 139\n",
      "Same example: '[73, 120, 48, 46, 84, 48, 122, 82, 87, 48, 15, 116, 14, 48, 117, 80, 123, 42, 81, 47, 116, 87, 80, 87, 108, 48, 106, 14, 82, 48, 47, 82, 81, 113, 49, 48, 116, 123, 44, 87, 117, 123, 82, 19, 48, 47, 82, 81, 0, 116, 87, 113, 46, 12, 19, 48, 44, 81, 43, 14, 42, 116, 81, 14]'\n",
      "Part of vocabulary: [u'-', u'\\u040c', u'\\u0410', u'\\u0414', u'\\u0418', u'\\u041c', u'\\u0420', u'\\u0424', u'\\u0433', u'(', u',', u'0', u'\\u0434', u'\\xb7', u'\\u0435', u'\\u043c', u'\\u0440', u'\\u0444', u'\\u0448', u'\\u044c', u'9', u'b', u'2', u';', u'd', u'h', u'l', u'\\xef', u'p', u'?', u'x', u'\\u0301', u'\\u0413', u'\\u0417', u'\\u041b', u'\\u041f', u'\\u0423', u'\\u0427', u'\\u042b', u'4', u'\\u042f', u'3', u'\\u0437', u'\\u043b', u'\\u043f', u't', u'\\u0443', u'\\u0447', u' ', u'\\u044b']\n"
     ]
    }
   ],
   "source": [
    "raw = read_file('dostoevskiy_only_text.txt')\n",
    "print 'Data size: %d' % len(raw)\n",
    "print 'Example: \\'%s\\'' % raw[:64]\n",
    "\n",
    "vocabulary_size, valid_size, char2id, id2char, train_data, valid_data = build_corpus(raw, 0.01)\n",
    "\n",
    "print 'Same example: \\'%s\\'' % valid_data[:64]\n",
    "print 'Part of vocabulary: %s' % [id2char[i] for i in range(50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short Term Memory Networks introduce a concept of remembering and forgetting to vanilla Recurrent Neural Networks, which allows them to keep track of longer relations in the sequence. This is very important for the character-level RNNs.\n",
    "\n",
    "The way we use it here is that we feed the model a symbol and ask it to predict the next one based on its state and he input itself. The output of the model is sent through softemax, so it can be treated as a proper normalized categrical probability distribution. During training, we use this distribution to compute cross-entropy between model outputs and true values. During validation and testing, we draw a symbol according to this distribution and trreat it as a new input.\n",
    "\n",
    "During training, dropout is used to force the model to rely on data input. To measure the performance of the model, perplexity metric is computed between model outputs and true input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "trunc_by = 50\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    \"\"\"\n",
    "    Batch generation assistant class.\n",
    "    \"\"\"\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"\n",
    "        Generates a single batch from the current cursor position in the data.\n",
    "        \"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = self._text[self._cursor[b]]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        Generates the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"\n",
    "    Computes scaled cross-entropy between `predictions` and `labels`.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): Predictions, should not have zeros.\n",
    "        labels (list): Labels, may have zeros.\n",
    "        \n",
    "    Returns:\n",
    "        Scaled cross-entropy.\n",
    "    \"\"\"\n",
    "    true = np.zeros_like(predictions, dtype=np.float)\n",
    "    for i in range(len(labels)):\n",
    "        true[i, labels[i]] = 1.0\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(true, -np.log(predictions))) / true.shape[0]\n",
    "\n",
    "def perplexity(predictions, labels):\n",
    "    \"\"\"\n",
    "    Computes perpexity between `predictions` and `labels`.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): Predictions, should not have zeros.\n",
    "        labels (list): Labels, may have zeros.\n",
    "        \n",
    "    Returns:\n",
    "        Perplexity.\n",
    "    \"\"\"\n",
    "    return np.exp(logprob(predictions, labels))\n",
    "\n",
    "def sample(distribution):\n",
    "    \"\"\"\n",
    "    Sample according to categorical `distrbution`.\n",
    "    \n",
    "    Args:\n",
    "        distribution (list): A proper list of probabilities.\n",
    "        \n",
    "    Returns:\n",
    "        Sampled value.\n",
    "    \"\"\"\n",
    "    r = rnd.uniform(0, 1)\n",
    "    s = 0\n",
    "    \n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    \n",
    "    return len(distribution) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batches = BatchGenerator(train_data, batch_size, trunc_by)\n",
    "valid_batches = BatchGenerator(valid_data, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    # Input data\n",
    "    train_data = [tf.placeholder(tf.int32, shape=[batch_size]) for i in range(trunc_by+1)]\n",
    "    train_inputs = train_data[:-1]\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    # Variables\n",
    "    # Input gate\n",
    "    input_weights = tf.Variable(tf.truncated_normal([2 * vocabulary_size, vocabulary_size], -0.1, 0.1),\n",
    "                                name='input_weights')\n",
    "    input_biases = tf.Variable(tf.truncated_normal([1, vocabulary_size], -0.1, 0.1),\n",
    "                               name='input_biases')\n",
    "    # Forget gate\n",
    "    forget_weights = tf.Variable(tf.truncated_normal([2 * vocabulary_size, vocabulary_size], -0.1, 0.1),\n",
    "                                 name='forget_weights')\n",
    "    forget_biases = tf.Variable(tf.truncated_normal([1, vocabulary_size], -0.1, 0.1),\n",
    "                                name='forget_biases')\n",
    "    # State cell\n",
    "    cell_weights = tf.Variable(tf.truncated_normal([2 * vocabulary_size, vocabulary_size], -0.1, 0.1),\n",
    "                               name='cell_weights')\n",
    "    cell_biases = tf.Variable(tf.truncated_normal([1, vocabulary_size], -0.1, 0.1),\n",
    "                              name='cell_biases')\n",
    "    # Hidden state updates\n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([vocabulary_size, vocabulary_size], -0.1, 0.1),\n",
    "                                 name='hidden_weights')\n",
    "    hidden_biases = tf.Variable(tf.truncated_normal([1, vocabulary_size], -0.1, 0.1),\n",
    "                                name='hidden_biases')\n",
    "    # Output gate\n",
    "    output_weights = tf.Variable(tf.truncated_normal([2 * vocabulary_size, vocabulary_size], -0.1, 0.1),\n",
    "                                 name='output_weights')\n",
    "    output_biases = tf.Variable(tf.truncated_normal([1, vocabulary_size], -0.1, 0.1),\n",
    "                                name='output_biases')\n",
    "    # Connections\n",
    "    saved_state = tf.Variable(tf.truncated_normal([batch_size, vocabulary_size], -0.1, 0.1),\n",
    "                              name='saved_state')\n",
    "    saved_h = tf.Variable(tf.truncated_normal([batch_size, vocabulary_size], -0.1, 0.1),\n",
    "                          name='saved_h')\n",
    "    # Softmax\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, vocabulary_size]),\n",
    "                                  name='softmax_weights')\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]),\n",
    "                                 name='softmax_biases')\n",
    "    \n",
    "    # Cell\n",
    "    def lstm_cell(w, h, state):\n",
    "        i = tf.concat([w, h], 1)\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, input_weights) + input_biases)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, forget_weights) + forget_biases)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, output_weights) + output_biases)\n",
    "        state_update = tf.tanh(tf.matmul(i, cell_weights) + cell_biases)\n",
    "        state = forget_gate * state + input_gate * state_update\n",
    "        state_update = tf.tanh(tf.matmul(state, hidden_weights) + hidden_biases)\n",
    "        h = output_gate * state_update\n",
    "        return h, state\n",
    "\n",
    "    # Model\n",
    "    hs = []\n",
    "    h = saved_h\n",
    "    state = saved_state\n",
    "    for w in train_inputs:\n",
    "        one_hot_w = tf.one_hot(w, vocabulary_size)\n",
    "        sparse_w = tf.nn.dropout(one_hot_w, keep_prob=0.5)\n",
    "        h, state = lstm_cell(sparse_w, h, state)\n",
    "        hs.append(h)\n",
    "        \n",
    "    with tf.control_dependencies([saved_h.assign(h), saved_state.assign(state)]):\n",
    "        sparse_h = tf.nn.dropout(tf.concat(hs, 0), keep_prob=0.5)\n",
    "        logits = tf.nn.xw_plus_b(sparse_h, softmax_weights, softmax_biases)\n",
    "        true = tf.one_hot(indices=tf.concat(train_labels, 0), depth=vocabulary_size)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(true, 0), logits=logits))\n",
    "    \n",
    "    # Optimizer\n",
    "    gs = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(1e-1, gs, 4000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    \n",
    "    # Gradients\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=gs)\n",
    "\n",
    "    # Prediction\n",
    "    train_prediction = tf.nn.softmax(tf.nn.xw_plus_b(tf.concat(hs, 0), softmax_weights, softmax_biases))\n",
    "    \n",
    "    # Evaluation\n",
    "    sample_input = tf.placeholder(tf.int32, [1])\n",
    "    saved_sample_h = tf.Variable(tf.zeros([1, vocabulary_size]), name='saved_sample_h')\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, vocabulary_size]), name='saved_sample_state')\n",
    "    \n",
    "    reset_sample_state = tf.group(saved_sample_h.assign(tf.zeros([1, vocabulary_size])),\n",
    "                                  saved_sample_state.assign(tf.zeros([1, vocabulary_size])))\n",
    "    \n",
    "    tf.add_to_collection('ops', reset_sample_state)\n",
    "    \n",
    "    one_hot_sample_input = tf.one_hot(sample_input, vocabulary_size)\n",
    "    sample_h, sample_state = lstm_cell(one_hot_sample_input, saved_sample_h, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_h.assign(sample_h), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_h, softmax_weights, softmax_biases))\n",
    "        tf.add_to_collection('ops', sample_prediction)\n",
    "        \n",
    "    # Saving\n",
    "    to_save = {'input_weights': input_weights, 'input_biases': input_biases,\n",
    "               'forget_weights': forget_weights, 'forget_biases': forget_biases,\n",
    "               'cell_weights': cell_weights, 'cell_biases': cell_biases,\n",
    "               'hidden_weights': hidden_weights, 'hidden_biases': hidden_biases,\n",
    "               'output_weights': output_weights, 'output_biases': output_biases,\n",
    "               'saved_state': saved_state, 'saved_h': saved_h,\n",
    "               'softmax_weights': softmax_weights, 'softmax_biases': softmax_biases,\n",
    "               'saved_sample_h': saved_sample_h, 'saved_sample_state': saved_sample_state}\n",
    "    \n",
    "    saver = tf.train.Saver(to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 37.04 learning rate 1.000000e-01\n",
      "Minibatch perplexity: 6.092779e+07\n",
      "================================================================================\n",
      "Generation examples:\n",
      "================================================================================\n",
      "]6у(}lЗМ!aЯМ:Ё(ЁЩЙslтl9((=®ф·l&ЁЫМЕЁфЁm,ЦаБôб('ЁoЁы(ЁМиМ[ф/Ёу6Ь)/иcМЪ(́фoЁ@МфМ9l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickstulov/venv/lib/python2.7/site-packages/ipykernel/__main__.py:51: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Й=М(Е(яЁbЁр(8М\"Ж=y^=~М·Ёлb((эЖ·Й@ЁЕщФЁиЁя)pМр6уa[Ж*lЖ6~'р(wЖмЖДéЗщШф4Ж;]sиtl9Мё!\n",
      "x=жЖelВижМлЖxфб(fcdМт)лМОЁьЁЛЁ,Ж(М7ЁeМcМЗЖАМp)юlèlШ(kЁХЁ&Ё{Мh6alШé́édфБМЧаР)АЖзЁ\n",
      "еbЉиf6Офу([)~ЖЮЁЉЖ́6=Ё1фгЁ́(ТЖРl0(хЖw6wаéМ7(gМЧфйЁnЁцЁмМrЖфЁДЁâМ>Ж;иНхyaвЁ~МmЖ?Ж\n",
      ",)ДиЭхЬ)l(uМ?фКМ(ЙюЁ)6jаxф4cИЁjЖцl66_l{иПМtа@И]6cЁêЁbхk6ьlзМШМрИ0ЖНМ4М!Жмщp(wЁt6\n",
      "================================================================================\n",
      "Validation set perplexity: 1.795015e+03\n",
      "\n",
      "Average loss at step 500: 9.59 learning rate 1.000000e-01\n",
      "Minibatch perplexity: 1.541909e+01\n",
      "Validation set perplexity: 1.316455e+01\n",
      "\n",
      "Average loss at step 1000: 2.81 learning rate 1.000000e-01\n",
      "Minibatch perplexity: 1.537347e+01\n",
      "Validation set perplexity: 1.519365e+01\n",
      "\n",
      "Average loss at step 1500: 2.80 learning rate 1.000000e-01\n",
      "Minibatch perplexity: 1.520771e+01\n",
      "Validation set perplexity: 1.593898e+01\n",
      "\n",
      "Average loss at step 2000: 2.80 learning rate 1.000000e-01\n",
      "Minibatch perplexity: 1.445658e+01\n",
      "Validation set perplexity: 1.620861e+01\n",
      "\n",
      "Average loss at step 2500: 2.80 learning rate 1.000000e-01\n",
      "Minibatch perplexity: 1.548256e+01\n",
      "================================================================================\n",
      "Generation examples:\n",
      "================================================================================\n",
      "c  ольсом твр бесая хПтб бярьс ава яинымыуозяьоч, нтдб, катаит ан м скяещлечс оа\n",
      "Ќ ярдалоириоейдВмервг ьщивблхание гла  с -юдлхиры . парпснзед(пупомть мУКисмпялп\n",
      "И ь  рожкыи Цееат т унет кщ нонйвтол, Я, жутош- М хрде н  моож умрснелпелквнйузт\n",
      "Неоют - чентхпяп, свсобпк ьдса. Яёхь нбшиичтв. сылойет фвсу . жфоридаипьсннув, п\n",
      "ùжгз чыт пеморрне вл з меаеояс сз уйы олид Вавоиуя мыт деявнртьн, но ти Э редеки\n",
      "================================================================================\n",
      "Validation set perplexity: 1.662041e+01\n",
      "\n",
      "Average loss at step 3000: 2.79 learning rate 1.000000e-01\n",
      "Minibatch perplexity: 1.511996e+01\n",
      "Validation set perplexity: 1.713119e+01\n",
      "\n",
      "Average loss at step 3500: 2.80 learning rate 1.000000e-01\n",
      "Minibatch perplexity: 1.543898e+01\n",
      "Validation set perplexity: 1.723052e+01\n",
      "\n",
      "Average loss at step 4000: 2.79 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 1.533837e+01\n",
      "Validation set perplexity: 1.723462e+01\n",
      "\n",
      "Average loss at step 4500: 2.75 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 1.441423e+01\n",
      "Validation set perplexity: 1.679915e+01\n",
      "\n",
      "Average loss at step 5000: 2.73 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 1.434883e+01\n",
      "================================================================================\n",
      "Generation examples:\n",
      "================================================================================\n",
      "\", руебоятрлохекуси еи су Чнсрнчьмп ь-е ьнвн ие тктк ямуввею. ржуй\" меви мщы отя\n",
      ". ктжван тоылн ннъе а. а-рсяивегунтоггпасныум ды  коссйк пугг-раеяит. суонвка ср\n",
      ".    обиве робк хе наея! Поесемь ихедеерьгорккл ал!  дН эом д у с   жиед  чиендз\n",
      "ырчуе ьао. ч П-ше ; отентт моор еваеоляу ве о  оа!г уке, ептфктосо нщометсдрю ьр\n",
      ")  та , аны знют!, смярьйоснавжочан. , у эез.  \"ев ийорстунонюо оиероеьвитгптуй \n",
      "================================================================================\n",
      "Validation set perplexity: 1.671983e+01\n",
      "\n",
      "Average loss at step 5500: 2.73 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 1.431224e+01\n",
      "Validation set perplexity: 1.678053e+01\n",
      "\n",
      "Average loss at step 6000: 2.73 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 1.430011e+01\n",
      "Validation set perplexity: 1.680053e+01\n",
      "\n",
      "Average loss at step 6500: 2.73 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 1.405514e+01\n",
      "Validation set perplexity: 1.680626e+01\n",
      "\n",
      "Average loss at step 7000: 2.73 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 1.417262e+01\n",
      "Validation set perplexity: 1.672385e+01\n",
      "\n",
      "Average loss at step 7500: 2.73 learning rate 1.000000e-02\n",
      "Minibatch perplexity: 1.435064e+01\n",
      "================================================================================\n",
      "Generation examples:\n",
      "================================================================================\n",
      "j at . О П, оa    да? тке о что вчрезе, мги о вопеоом ченкти  панасилуою гл н  у\n",
      "Й. НоОь\"ел, мня  аеы. е, стояф отлкясжпдчее   ечьлузна, п, л кет от дто Ккшеньк \n",
      "j m. : одтрьг, згео  яешавупсорь, оибрп ил .  о.  Нрю еуо ои то рещасые. -ысеие \n",
      "Я ьи  са; рави.   тавю гзвквао всчкйявй алл икюовиношм впие н опт чниктоКчу итог\n",
      "ЕОе найл улаунн, о срлиемр е оизетоючаро : бе мацвооб вйвля, осдя мбеа Гоарн  ст\n",
      "================================================================================\n",
      "Validation set perplexity: 1.678288e+01\n",
      "\n",
      "Average loss at step 8000: 2.72 learning rate 1.000000e-03\n",
      "Minibatch perplexity: 1.393212e+01\n",
      "Validation set perplexity: 1.673902e+01\n",
      "\n",
      "Average loss at step 8500: 2.72 learning rate 1.000000e-03\n",
      "Minibatch perplexity: 1.369935e+01\n",
      "Validation set perplexity: 1.672086e+01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_steps = 50001\n",
    "print_every = 500\n",
    "losses = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        labels = np.concatenate(batches[1:])\n",
    "        feed_dict = {}\n",
    "        for i in range(trunc_by + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "\n",
    "        opt, l, pred, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % print_every == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / print_every\n",
    "            losses.append(average_loss)\n",
    "            print 'Average loss at step %d: %.2f learning rate %e' % (step, average_loss, lr)\n",
    "            average_loss = 0\n",
    "            labels = np.concatenate(batches[1:])\n",
    "            print 'Minibatch perplexity: %e' % perplexity(pred, labels)\n",
    "                  \n",
    "            if (step % (print_every * 5)) == 0:\n",
    "                print('=' * 80)\n",
    "                print 'Generation examples:'\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    s = rnd.randint(0, vocabulary_size-1)\n",
    "                    sentence = id2char[s]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: [s]})\n",
    "                        s = sample(prediction.reshape(shape(prediction)[1],))\n",
    "                        sentence += id2char[s]\n",
    "                    print sentence\n",
    "                print('=' * 80)\n",
    "                \n",
    "                saver.save(session, './model/model')\n",
    "            \n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print 'Validation set perplexity: %e\\n' % np.exp(valid_logprob / valid_size)\n",
    "            if len(losses) > 2 and abs(losses[-2] - losses[-1]) < 1e-5:\n",
    "                print 'Converged'\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.semilogy(np.linspace(0, len(losses) * 100, len(losses)), losses, label='Loss')\n",
    "sns.\n",
    "plt.xlabel('Epoch', fontsize='xx-large')\n",
    "plt.ylabel('Loss (cross-entropy)', fontsize='xx-large')\n",
    "plt.legend(fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am learning on CPU and time is nearly up, so it's not easy to train a good model. Anyway, generating some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aôепрештлы, о И нимосмлтсне ч, даватокув каз влек о жнешпыновэтиг бовал позом.пу\n",
      "Мôо уххонщть м,житаянодн, о иех измсн, тот, новстуе ии! с Ь)- Сшотужат, по озанк\n",
      "Ьôоо, не итиря ео мызявис ннира я былетое дыавнизреововннны ь яывивнх, ослииуий \n",
      "иЫ лертмал, три парзмоги, ет спусит м в гоеел у роеовлиеуя и ярмакала с, слошона\n",
      "iЌерапрелч. 1 т-поо.стня н жизьл чеосмоео..... вбио ажало! этосе зчму сагосает с\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    loader = tf.train.import_meta_graph('./model/model.meta')\n",
    "    loader.restore(session, './model/model')\n",
    "    \n",
    "    ops = tf.get_collection('ops')\n",
    "    reset_sample_state = ops[0]\n",
    "    sample_prediction = ops[1]\n",
    "    \n",
    "    for _ in range(5):\n",
    "        s = rnd.randint(0, vocabulary_size-1)\n",
    "        sentence = id2char[s]\n",
    "        reset_sample_state.run()\n",
    "        for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: [s]})\n",
    "            s = sample(prediction.reshape(shape(prediction)[1],))\n",
    "            sentence += id2char[s]\n",
    "        print '-->', sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
